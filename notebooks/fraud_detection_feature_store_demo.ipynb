{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Fraud Detection with Feature Store on OpenShift AI 3.2\n\nThis notebook demonstrates the use of **Feature Store** (Feast) on **Red Hat OpenShift AI 3.2** for **bank fraud detection**.\n\n## Prerequisites\n\n- Workbench created in the `fraud-detection-ml` project\n- Feature Store `fraud_detection` selected in the workbench configuration\n- Data Connection to MinIO configured (for historical features)\n\n## Workflow\n1. Connect to the Feature Store (using the auto-mounted client config)\n2. Explore registered features\n3. Retrieve historical features for training\n4. Train a fraud detection model\n5. Real-time prediction via the online store"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Install dependencies"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install -q feast[postgres] scikit-learn pandas pyarrow s3fs boto3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Connect to the Feature Store\n\nThe Feast client configuration is **auto-mounted** by RHOAI when you select the Feature Store in the workbench settings. The config file is at `/opt/app-root/src/feast-config/<project_name>`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\nfrom feast import FeatureStore\n\n# The Feast client config is auto-mounted by RHOAI when you select\n# the Feature Store in the workbench settings.\nfeast_config_dir = \"/opt/app-root/src/feast-config\"\n\nif os.path.isdir(feast_config_dir):\n    config_files = [\n        os.path.join(feast_config_dir, f)\n        for f in os.listdir(feast_config_dir)\n        if os.path.isfile(os.path.join(feast_config_dir, f))\n    ]\nelse:\n    config_files = []\n\nif config_files:\n    fs_yaml = config_files[0]\n    print(f\"Using auto-mounted config: {fs_yaml}\")\n    with open(fs_yaml) as f:\n        print(f.read())\n    store = FeatureStore(fs_yaml_file=fs_yaml)\nelse:\n    raise FileNotFoundError(\n        f\"No Feast config found in {feast_config_dir}. \"\n        \"Make sure you selected the Feature Store when creating the workbench.\"\n    )\n\nprint(f\"\\nProject: {store.project}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Explore registered features\n\nThe features, entities, and feature views are already registered via `feast apply` (done during deployment)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=== Entities ===\")\nfor entity in store.list_entities():\n    print(f\"  - {entity.name}\")\n\nprint(\"\\n=== Feature Views ===\")\nfor fv in store.list_feature_views():\n    print(f\"  - {fv.name} ({len(fv.features)} features, TTL={fv.ttl})\")\n    for feature in fv.features:\n        print(f\"      {feature.name}: {feature.dtype}\")\n\nprint(\"\\n=== On-Demand Feature Views ===\")\nfor odfv in store.list_on_demand_feature_views():\n    print(f\"  - {odfv.name}\")\n    for feature in odfv.features:\n        print(f\"      {feature.name}: {feature.dtype}\")\n\nprint(\"\\n=== Data Sources ===\")\nfor ds in store.list_data_sources():\n    print(f\"  - {ds.name} ({type(ds).__name__})\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 4. Retrieve historical features (Training)\n\nWe retrieve features from the **offline store** (Parquet files on MinIO/S3) to train a fraud detection model.\n\nThis requires the MinIO Data Connection configured in the workbench (provides `AWS_*` environment variables)."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import numpy as np\nimport pandas as pd\n\nnp.random.seed(42)\n\nnow = pd.Timestamp.now()\ncustomer_ids = [f\"C{str(i).zfill(5)}\" for i in range(1, 51)]\nN_TRANSACTIONS = 2000\n\n# Simulate labeled transaction data\nentity_df = pd.DataFrame({\n    \"customer_id\": np.random.choice(customer_ids, N_TRANSACTIONS),\n    \"event_timestamp\": [now - pd.Timedelta(hours=np.random.randint(1, 24)) for _ in range(N_TRANSACTIONS)],\n    \"transaction_amount\": np.round(np.random.exponential(200, N_TRANSACTIONS), 2),\n    \"is_foreign_transaction\": np.random.choice([0, 1], N_TRANSACTIONS, p=[0.85, 0.15]),\n})\n\n# Features to retrieve\nfeature_refs = [\n    \"customer_profile:age\",\n    \"customer_profile:account_age_days\",\n    \"customer_profile:credit_limit\",\n    \"customer_profile:num_cards\",\n    \"transaction_stats:avg_transaction_amount_30d\",\n    \"transaction_stats:num_transactions_7d\",\n    \"transaction_stats:num_transactions_1d\",\n    \"transaction_stats:max_transaction_amount_7d\",\n    \"transaction_stats:num_foreign_transactions_30d\",\n    \"transaction_stats:num_declined_transactions_7d\",\n    \"fraud_risk_features:amount_ratio_to_avg\",\n    \"fraud_risk_features:amount_ratio_to_max\",\n    \"fraud_risk_features:risk_score\",\n]\n\n# Retrieve historical features from the offline store (Parquet on S3)\nprint(\"Retrieving historical features from S3...\")\ntraining_df = store.get_historical_features(\n    entity_df=entity_df,\n    features=feature_refs,\n).to_df()\n\nprint(f\"Training dataset: {training_df.shape[0]} rows, {training_df.shape[1]} columns\")\ntraining_df.head(10)"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 5. Train a fraud detection model"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Generate simulated fraud labels\n# Transactions with a high risk_score are more likely to be fraudulent\ntraining_df = training_df.dropna()\nfraud_probability = 1 / (1 + np.exp(-(training_df[\"risk_score\"] - 1.5) * 3))\ntraining_df[\"is_fraud\"] = (np.random.random(len(training_df)) < fraud_probability).astype(int)\n\nprint(f\"Fraud distribution:\")\nprint(training_df[\"is_fraud\"].value_counts())\nprint(f\"Fraud rate: {training_df['is_fraud'].mean():.2%}\")\n\n# Prepare features for the model\nmodel_features = [\n    \"age\", \"account_age_days\", \"credit_limit\", \"num_cards\",\n    \"avg_transaction_amount_30d\", \"num_transactions_7d\", \"num_transactions_1d\",\n    \"max_transaction_amount_7d\", \"num_foreign_transactions_30d\",\n    \"num_declined_transactions_7d\",\n    \"transaction_amount\", \"is_foreign_transaction\",\n    \"amount_ratio_to_avg\", \"amount_ratio_to_max\", \"risk_score\",\n]\n\nX = training_df[model_features]\ny = training_df[\"is_fraud\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a Random Forest\nclf = RandomForestClassifier(n_estimators=100, random_state=42)\nclf.fit(X_train, y_train)\n\n# Evaluate\ny_pred = clf.predict(X_test)\nprint(\"\\n=== Classification Report ===\")\nprint(classification_report(y_test, y_pred, target_names=[\"Legitimate\", \"Fraud\"]))\n\nprint(\"=== Confusion Matrix ===\")\nprint(confusion_matrix(y_test, y_pred))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Feature importance\nimport matplotlib.pyplot as plt\n\nimportances = pd.Series(clf.feature_importances_, index=model_features).sort_values(ascending=True)\n\nfig, ax = plt.subplots(figsize=(10, 6))\nimportances.plot(kind=\"barh\", ax=ax)\nax.set_title(\"Feature importance for fraud detection\")\nax.set_xlabel(\"Importance\")\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 6. Real-time prediction via the Online Store\n\nSimulate an incoming transaction: retrieve the customer's features from the **online store** (PostgreSQL) in real-time, then apply the model."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Simulate a suspicious transaction\ntest_customer = \"C00042\"\ntransaction = {\n    \"transaction_amount\": 4500.00,  # high amount\n    \"is_foreign_transaction\": 1,     # foreign transaction\n}\n\nprint(f\"Incoming transaction for customer {test_customer}:\")\nprint(f\"  Amount: {transaction['transaction_amount']} EUR\")\nprint(f\"  Foreign transaction: {'Yes' if transaction['is_foreign_transaction'] else 'No'}\")\n\n# Retrieve features in real-time from PostgreSQL (online store)\nonline_features = store.get_online_features(\n    entity_rows=[\n        {\n            \"customer_id\": test_customer,\n            **transaction,\n        }\n    ],\n    features=feature_refs,\n).to_dict()\n\nprint(\"\\nFeatures retrieved from the online store (PostgreSQL):\")\nfor key, values in online_features.items():\n    if key != \"customer_id\":\n        print(f\"  {key}: {values[0]}\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Build feature vector for prediction\nfeature_vector = {}\nfeature_vector.update(online_features)\nfeature_vector.update({k: [v] for k, v in transaction.items()})\n\npredict_df = pd.DataFrame(feature_vector)\n\n# Keep only the model features (in the right order)\navailable_features = [f for f in model_features if f in predict_df.columns]\npredict_input = predict_df[available_features]\n\n# Predict\nprediction = clf.predict(predict_input)[0]\nprobability = clf.predict_proba(predict_input)[0]\n\nprint(\"\\n\" + \"=\" * 50)\nif prediction == 1:\n    print(f\"FRAUD ALERT - Probability: {probability[1]:.1%}\")\n    print(\"Action: Transaction blocked for review\")\nelse:\n    print(f\"Legitimate transaction - Fraud probability: {probability[1]:.1%}\")\n    print(\"Action: Transaction approved\")\nprint(\"=\" * 50)"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 7. Architecture Summary\n\n```\n                    OpenShift AI 3.2\n    +---------------------------------------------+\n    |                                             |\n    |   +-------------+   +-------------------+   |\n    |   |  Notebook    |   |  Feature Store    |   |\n    |   |  (Workbench) |-->|  (Feast Operator) |   |\n    |   +-------------+   +---------+---------+   |\n    |                               |              |\n    |              +----------------+----------+   |\n    |              v                v          v   |\n    |   +----------------+ +------------+ +-----+  |\n    |   | Offline Store  | | Online     | |  S3 |  |\n    |   | (Parquet/MinIO)| | Store      | |     |  |\n    |   |                | | (Postgres) | | Reg |  |\n    |   | Training       | | Serving    | |+Data|  |\n    |   +----------------+ +------------+ +-----+  |\n    |                                             |\n    +---------------------------------------------+\n```\n\n| Component | Technology | Usage |\n|-----------|------------|-------|\n| Offline Store | Parquet on S3 (MinIO) | Historical features for training |\n| Online Store | PostgreSQL | Real-time features for inference |\n| Registry | S3 (MinIO) | Feature metadata |\n| Feature Server | Feast (RHOAI Operator) | gRPC/REST API to serve features |\n| Notebook | RHOAI Workbench | Development and experimentation |"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}