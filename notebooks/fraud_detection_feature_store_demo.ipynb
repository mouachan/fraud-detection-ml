{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Fraud Detection with Feature Store on OpenShift AI 3.2\n\nThis notebook demonstrates the **full ML lifecycle** using **Feature Store** (Feast), **Model Registry**, and **Model Serving** (KServe/OVMS) on **Red Hat OpenShift AI 3.2** for real-time bank fraud detection.\n\n## Prerequisites\n\n- Workbench created in the `fraud-detection-ml` project\n- Feature Store `fraud_detection` selected in the workbench configuration\n- Data Connection to MinIO configured (for historical features and model storage)\n- Model Registry `fraud-detection` available\n\n## Workflow\n1. Connect to the Feature Store (auto-mounted client config)\n2. Explore registered features\n3. Retrieve historical features for training\n4. Train a fraud detection model (RandomForest)\n5. Real-time prediction via the online store (local model)\n6. **Export model to ONNX**\n7. **Upload model to MinIO (S3)**\n8. **Register in the Model Registry**\n9. **Deploy for serving (KServe + OVMS)**\n10. **End-to-end inference: Feast + Model Serving**"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Connect to the Feature Store\n\nThe Feast client configuration is **auto-mounted** by RHOAI when you select the Feature Store in the workbench settings. The config file is at `/opt/app-root/src/feast-config/<project_name>`.\n\n> **Note:** If you use the custom workbench image (`fraud-detection-datascience`), all dependencies are pre-installed. No `pip install` needed."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Only needed if NOT using the custom workbench image\n# !pip install -q feast[postgres] scikit-learn pandas pyarrow s3fs boto3 skl2onnx onnxruntime model-registry"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Connect to the Feature Store"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\nfrom feast import FeatureStore\n\n# The Feast client config is auto-mounted by RHOAI when you select\n# the Feature Store in the workbench settings.\nfeast_config_dir = \"/opt/app-root/src/feast-config\"\n\nif os.path.isdir(feast_config_dir):\n    config_files = [\n        os.path.join(feast_config_dir, f)\n        for f in os.listdir(feast_config_dir)\n        if os.path.isfile(os.path.join(feast_config_dir, f))\n    ]\nelse:\n    config_files = []\n\nif config_files:\n    fs_yaml = config_files[0]\n    print(f\"Using auto-mounted config: {fs_yaml}\")\n    with open(fs_yaml) as f:\n        print(f.read())\n    store = FeatureStore(fs_yaml_file=fs_yaml)\nelse:\n    raise FileNotFoundError(\n        f\"No Feast config found in {feast_config_dir}. \"\n        \"Make sure you selected the Feature Store when creating the workbench.\"\n    )\n\nprint(f\"\\nProject: {store.project}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Explore registered features\n\nThe features, entities, and feature views are already registered via `feast apply` (done during deployment)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=== Entities ===\")\nfor entity in store.list_entities():\n    print(f\"  - {entity.name}\")\n\nprint(\"\\n=== Feature Views ===\")\nfor fv in store.list_feature_views():\n    print(f\"  - {fv.name} ({len(fv.features)} features, TTL={fv.ttl})\")\n    for feature in fv.features:\n        print(f\"      {feature.name}: {feature.dtype}\")\n\nprint(\"\\n=== On-Demand Feature Views ===\")\nfor odfv in store.list_on_demand_feature_views():\n    print(f\"  - {odfv.name}\")\n    for feature in odfv.features:\n        print(f\"      {feature.name}: {feature.dtype}\")\n\nprint(\"\\n=== Data Sources ===\")\nfor ds in store.list_data_sources():\n    print(f\"  - {ds.name} ({type(ds).__name__})\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 4. Retrieve historical features (Training)\n\nWe retrieve features from the **offline store** (Parquet files on MinIO/S3) to train a fraud detection model.\n\nThis requires the MinIO Data Connection configured in the workbench (provides `AWS_*` environment variables)."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import numpy as np\nimport pandas as pd\n\nnp.random.seed(42)\n\nnow = pd.Timestamp.now()\ncustomer_ids = [f\"C{str(i).zfill(5)}\" for i in range(1, 51)]\nN_TRANSACTIONS = 500\n\n# Simulate labeled transaction data\nentity_df = pd.DataFrame({\n    \"customer_id\": np.random.choice(customer_ids, N_TRANSACTIONS),\n    \"event_timestamp\": [now - pd.Timedelta(hours=np.random.randint(1, 24)) for _ in range(N_TRANSACTIONS)],\n    \"transaction_amount\": np.round(np.random.exponential(200, N_TRANSACTIONS), 2),\n    \"is_foreign_transaction\": np.random.choice([0, 1], N_TRANSACTIONS, p=[0.85, 0.15]),\n})\n\n# Features to retrieve from offline store (materialized feature views only)\noffline_feature_refs = [\n    \"customer_profile:age\",\n    \"customer_profile:account_age_days\",\n    \"customer_profile:credit_limit\",\n    \"customer_profile:num_cards\",\n    \"transaction_stats:avg_transaction_amount_30d\",\n    \"transaction_stats:num_transactions_7d\",\n    \"transaction_stats:num_transactions_1d\",\n    \"transaction_stats:max_transaction_amount_7d\",\n    \"transaction_stats:num_foreign_transactions_30d\",\n    \"transaction_stats:num_declined_transactions_7d\",\n]\n\n# Retrieve historical features from the offline store (Parquet on S3)\nprint(f\"Retrieving historical features from S3 ({N_TRANSACTIONS} rows)...\")\ntraining_df = store.get_historical_features(\n    entity_df=entity_df,\n    features=offline_feature_refs,\n).to_df()\n\n# Compute on-demand features locally (avoids heavy Feast join for computed features)\ntraining_df[\"amount_ratio_to_avg\"] = (\n    training_df[\"transaction_amount\"] / training_df[\"avg_transaction_amount_30d\"].clip(lower=1)\n)\ntraining_df[\"amount_ratio_to_max\"] = (\n    training_df[\"transaction_amount\"] / training_df[\"max_transaction_amount_7d\"].clip(lower=1)\n)\ntraining_df[\"risk_score\"] = (\n    training_df[\"amount_ratio_to_avg\"] * 0.4\n    + training_df[\"amount_ratio_to_max\"] * 0.3\n    + training_df[\"is_foreign_transaction\"] * 0.3\n)\n\n# Full feature refs (for online store calls later)\nfeature_refs = offline_feature_refs + [\n    \"fraud_risk_features:amount_ratio_to_avg\",\n    \"fraud_risk_features:amount_ratio_to_max\",\n    \"fraud_risk_features:risk_score\",\n]\n\nprint(f\"Training dataset: {training_df.shape[0]} rows, {training_df.shape[1]} columns\")\ntraining_df.head(10)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 5. Train a fraud detection model"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Generate simulated fraud labels\n# Transactions with a high risk_score are more likely to be fraudulent\ntraining_df = training_df.dropna()\nfraud_probability = 1 / (1 + np.exp(-(training_df[\"risk_score\"] - 1.5) * 3))\ntraining_df[\"is_fraud\"] = (np.random.random(len(training_df)) < fraud_probability).astype(int)\n\nprint(f\"Fraud distribution:\")\nprint(training_df[\"is_fraud\"].value_counts())\nprint(f\"Fraud rate: {training_df['is_fraud'].mean():.2%}\")\n\n# Prepare features for the model\nmodel_features = [\n    \"age\", \"account_age_days\", \"credit_limit\", \"num_cards\",\n    \"avg_transaction_amount_30d\", \"num_transactions_7d\", \"num_transactions_1d\",\n    \"max_transaction_amount_7d\", \"num_foreign_transactions_30d\",\n    \"num_declined_transactions_7d\",\n    \"transaction_amount\", \"is_foreign_transaction\",\n    \"amount_ratio_to_avg\", \"amount_ratio_to_max\", \"risk_score\",\n]\n\nX = training_df[model_features]\ny = training_df[\"is_fraud\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a Random Forest\nclf = RandomForestClassifier(n_estimators=100, random_state=42)\nclf.fit(X_train, y_train)\n\n# Evaluate\ny_pred = clf.predict(X_test)\nprint(\"\\n=== Classification Report ===\")\nprint(classification_report(y_test, y_pred, target_names=[\"Legitimate\", \"Fraud\"]))\n\nprint(\"=== Confusion Matrix ===\")\nprint(confusion_matrix(y_test, y_pred))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Feature importance\nimport matplotlib.pyplot as plt\n\nimportances = pd.Series(clf.feature_importances_, index=model_features).sort_values(ascending=True)\n\nfig, ax = plt.subplots(figsize=(10, 6))\nimportances.plot(kind=\"barh\", ax=ax)\nax.set_title(\"Feature importance for fraud detection\")\nax.set_xlabel(\"Importance\")\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 6. Real-time prediction via the Online Store\n\nSimulate an incoming transaction: retrieve the customer's features from the **online store** (PostgreSQL) in real-time, then apply the model."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Simulate a suspicious transaction\ntest_customer = \"C00042\"\ntransaction = {\n    \"transaction_amount\": 4500.00,  # high amount\n    \"is_foreign_transaction\": 1,     # foreign transaction\n}\n\nprint(f\"Incoming transaction for customer {test_customer}:\")\nprint(f\"  Amount: {transaction['transaction_amount']} EUR\")\nprint(f\"  Foreign transaction: {'Yes' if transaction['is_foreign_transaction'] else 'No'}\")\n\n# Retrieve features in real-time from PostgreSQL (online store)\nonline_features = store.get_online_features(\n    entity_rows=[\n        {\n            \"customer_id\": test_customer,\n            **transaction,\n        }\n    ],\n    features=feature_refs,\n).to_dict()\n\nprint(\"\\nFeatures retrieved from the online store (PostgreSQL):\")\nfor key, values in online_features.items():\n    if key != \"customer_id\":\n        print(f\"  {key}: {values[0]}\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Build feature vector for prediction\nfeature_vector = {}\nfeature_vector.update(online_features)\nfeature_vector.update({k: [v] for k, v in transaction.items()})\n\npredict_df = pd.DataFrame(feature_vector)\n\n# Keep only the model features (in the right order)\navailable_features = [f for f in model_features if f in predict_df.columns]\npredict_input = predict_df[available_features]\n\n# Predict\nprediction = clf.predict(predict_input)[0]\nprobability = clf.predict_proba(predict_input)[0]\n\nprint(\"\\n\" + \"=\" * 50)\nif prediction == 1:\n    print(f\"FRAUD ALERT - Probability: {probability[1]:.1%}\")\n    print(\"Action: Transaction blocked for review\")\nelse:\n    print(f\"Legitimate transaction - Fraud probability: {probability[1]:.1%}\")\n    print(\"Action: Transaction approved\")\nprint(\"=\" * 50)"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Architecture Summary\n\n```\n                        OpenShift AI 3.2\n    +----------------------------------------------------------+\n    |                                                          |\n    |   +-------------+        +-------------------------+     |\n    |   |  Workbench   |        |  Feature Store          |     |\n    |   |  (Notebook)  |------->|  (Feast Operator)       |     |\n    |   +------+------+        +----------+--------------+     |\n    |          |                           |                    |\n    |          |          +---------------+-----------+         |\n    |          |          v               v           v         |\n    |          |  +---------------+ +-----------+  +--------+  |\n    |          |  | Offline Store | | Online    |  |   S3   |  |\n    |          |  | (Parquet/S3)  | | Store     |  | (MinIO)|  |\n    |          |  | Training      | | (Postgres)|  | Reg    |  |\n    |          |  +---------------+ +-----------+  | +Data  |  |\n    |          |                                   | +Models |  |\n    |          v                                   +--------+  |\n    |   +-------------+   +----------------+                   |\n    |   |   Model      |   | Model Serving  |                   |\n    |   |   Registry   |-->| (KServe/OVMS)  |                   |\n    |   +-------------+   +----------------+                   |\n    +----------------------------------------------------------+\n```"
  },
  {
   "cell_type": "markdown",
   "source": "## 8. Export the model to ONNX\n\nWe convert the trained RandomForest to ONNX format for serving with OpenVINO Model Server (OVMS).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Dependencies already installed in custom workbench image\n# !pip install -q skl2onnx onnxruntime model-registry",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from skl2onnx import convert_sklearn\nfrom skl2onnx.common.data_types import FloatTensorType\nimport onnxruntime as ort\n\n# Convert sklearn model to ONNX\ninitial_type = [(\"input\", FloatTensorType([None, len(model_features)]))]\nonnx_model = convert_sklearn(clf, initial_types=initial_type)\n\n# Save locally\nonnx_path = \"/tmp/fraud_model/1/model.onnx\"\nos.makedirs(os.path.dirname(onnx_path), exist_ok=True)\n\nwith open(onnx_path, \"wb\") as f:\n    f.write(onnx_model.SerializeToString())\n\nprint(f\"Model exported to {onnx_path}\")\nprint(f\"Model size: {os.path.getsize(onnx_path) / 1024:.1f} KB\")\n\n# Quick validation with ONNX Runtime\nsession = ort.InferenceSession(onnx_path)\ntest_input = X_test.iloc[:3].values.astype(np.float32)\nonnx_preds = session.run(None, {\"input\": test_input})\nprint(f\"\\nONNX validation - predictions: {onnx_preds[0].flatten()}\")\nprint(f\"Sklearn validation - predictions: {clf.predict(X_test.iloc[:3])}\")\nprint(\"ONNX export validated successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 9. Upload model to MinIO (S3)\n\nUpload the ONNX model to the `models` bucket in MinIO. OVMS expects the model at `<model_name>/<version>/model.onnx`.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import boto3\n\ns3 = boto3.client(\n    \"s3\",\n    endpoint_url=os.environ[\"AWS_ENDPOINT_URL\"],\n    aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n    aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n    region_name=os.environ.get(\"AWS_DEFAULT_REGION\", \"us-east-1\"),\n)\n\n# Ensure bucket exists\ntry:\n    s3.create_bucket(Bucket=\"models\")\nexcept s3.exceptions.BucketAlreadyOwnedByYou:\n    pass\nexcept Exception:\n    pass  # bucket may already exist\n\n# Upload: OVMS expects models/<model_name>/<version>/model.onnx\nmodel_s3_key = \"fraud-detection/1/model.onnx\"\ns3.upload_file(onnx_path, \"models\", model_s3_key)\n\nmodel_uri = f\"s3://models/{model_s3_key}\"\nprint(f\"Model uploaded to {model_uri}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 10. Register in the Model Registry\n\nRegister the model metadata in the RHOAI Model Registry. This stores the model name, version, S3 URI, format, and custom metadata (accuracy, features used, etc.).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from model_registry import ModelRegistry\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# Connect to the Model Registry\nregistry = ModelRegistry(\n    server_address=\"http://fraud-detection.rhoai-model-registries.svc.cluster.local\",\n    port=8080,\n    author=\"fraud-detection-notebook\",\n    is_secure=False,\n)\n\n# Compute metrics\naccuracy = accuracy_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\n# Register the model\nrm = registry.register_model(\n    \"fraud-detection\",\n    uri=\"s3://models/fraud-detection/1/model.onnx\",\n    version=\"1.0.0\",\n    description=\"Fraud detection model (RandomForest) trained with Feast features, exported to ONNX\",\n    model_format_name=\"onnx\",\n    model_format_version=\"1\",\n    storage_key=\"minio-data-connection\",\n    storage_path=\"fraud-detection/1/model.onnx\",\n    metadata={\n        \"accuracy\": str(round(accuracy, 4)),\n        \"f1_score\": str(round(f1, 4)),\n        \"framework\": \"sklearn\",\n        \"n_features\": str(len(model_features)),\n        \"features\": \",\".join(model_features),\n        \"feast_feature_views\": \"customer_profile,transaction_stats,fraud_risk_features\",\n    },\n)\n\nprint(f\"Model registered: {rm.name}\")\nprint(f\"  ID: {rm.id}\")\n\n# Verify\nmodel = registry.get_registered_model(\"fraud-detection\")\nversion = registry.get_model_version(\"fraud-detection\", \"1.0.0\")\nartifact = registry.get_model_artifact(\"fraud-detection\", \"1.0.0\")\nprint(f\"\\nVerification:\")\nprint(f\"  Registered Model: {model.name} (ID: {model.id})\")\nprint(f\"  Version: {version.name} (ID: {version.id})\")\nprint(f\"  Artifact URI: {artifact.uri}\")\nprint(f\"  Format: {artifact.model_format_name}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 11. Deploy the model for serving (KServe + OVMS)\n\nCreate an `InferenceService` that deploys the ONNX model from MinIO using OpenVINO Model Server. The model will be served via a REST/gRPC endpoint.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import subprocess\nimport json\nimport time\n\nNAMESPACE = \"fraud-detection-ml\"\n\n# Get model registry IDs for traceability labels\nregistered_model_id = model.id\nmodel_version_id = version.id\n\n# Create the InferenceService YAML\nisvc_yaml = f\"\"\"\napiVersion: serving.kserve.io/v1beta1\nkind: InferenceService\nmetadata:\n  name: fraud-detection\n  namespace: {NAMESPACE}\n  labels:\n    opendatahub.io/dashboard: \"true\"\n    modelregistry/registered-model-id: \"{registered_model_id}\"\n    modelregistry/model-version-id: \"{model_version_id}\"\n  annotations:\n    serving.kserve.io/deploymentMode: RawDeployment\n    openshift.io/display-name: \"Fraud Detection (ONNX)\"\nspec:\n  predictor:\n    model:\n      modelFormat:\n        name: onnx\n        version: \"1\"\n      runtime: kserve-ovms\n      storageUri: \"s3://models/fraud-detection\"\n      storage:\n        key: minio-data-connection\n        path: fraud-detection\n      resources:\n        requests:\n          cpu: 500m\n          memory: 512Mi\n        limits:\n          cpu: \"1\"\n          memory: 1Gi\n\"\"\"\n\n# Write and apply\nwith open(\"/tmp/isvc.yaml\", \"w\") as f:\n    f.write(isvc_yaml)\n\nresult = subprocess.run([\"oc\", \"apply\", \"-f\", \"/tmp/isvc.yaml\"], capture_output=True, text=True)\nprint(result.stdout)\nif result.returncode != 0:\n    print(f\"Error: {result.stderr}\")\n\n# Wait for the InferenceService to be ready\nprint(\"Waiting for InferenceService to be ready...\")\nfor i in range(30):\n    result = subprocess.run(\n        [\"oc\", \"get\", \"inferenceservice\", \"fraud-detection\", \"-n\", NAMESPACE,\n         \"-o\", \"jsonpath={.status.conditions[?(@.type=='Ready')].status}\"],\n        capture_output=True, text=True\n    )\n    if result.stdout == \"True\":\n        print(\"InferenceService is Ready!\")\n        break\n    print(f\"  Waiting... ({i+1}/30)\")\n    time.sleep(10)\n\n# Show status\nsubprocess.run([\"oc\", \"get\", \"inferenceservice\", \"-n\", NAMESPACE], capture_output=False)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 12. End-to-end inference: Feast online features + Model Serving\n\nThis is the full production flow:\n1. Receive a transaction (customer ID + transaction details)\n2. Fetch real-time features from Feast (online store)\n3. Send the feature vector to the OVMS endpoint\n4. Get the fraud prediction",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import requests\n\n# Get the inference endpoint\nresult = subprocess.run(\n    [\"oc\", \"get\", \"inferenceservice\", \"fraud-detection\", \"-n\", NAMESPACE,\n     \"-o\", \"jsonpath={.status.url}\"],\n    capture_output=True, text=True\n)\ninference_url = result.stdout.strip()\n\n# For RawDeployment, use the service URL directly\nif not inference_url:\n    inference_url = f\"http://fraud-detection-predictor.{NAMESPACE}.svc.cluster.local:8888\"\n\nprint(f\"Inference endpoint: {inference_url}\")\n\n# --- Simulate a suspicious transaction ---\ntest_customer = \"C00003\"\ntest_transaction = {\n    \"transaction_amount\": 8500.00,\n    \"is_foreign_transaction\": 1,\n}\n\nprint(f\"\\nIncoming transaction for customer {test_customer}:\")\nprint(f\"  Amount: {test_transaction['transaction_amount']} EUR\")\nprint(f\"  Foreign: {'Yes' if test_transaction['is_foreign_transaction'] else 'No'}\")\n\n# Step 1: Fetch online features from Feast\nonline_features = store.get_online_features(\n    entity_rows=[{\"customer_id\": test_customer, **test_transaction}],\n    features=feature_refs,\n).to_dict()\n\n# Step 2: Build the feature vector in the correct order\nfeature_values = []\nfor feat in model_features:\n    if feat in online_features:\n        feature_values.append(float(online_features[feat][0] or 0))\n    elif feat in test_transaction:\n        feature_values.append(float(test_transaction[feat]))\n    else:\n        feature_values.append(0.0)\n\nprint(f\"\\nFeature vector ({len(feature_values)} features): {[round(v, 2) for v in feature_values]}\")\n\n# Step 3: Call the OVMS endpoint (V2 inference protocol)\npayload = {\n    \"inputs\": [\n        {\n            \"name\": \"input\",\n            \"shape\": [1, len(model_features)],\n            \"datatype\": \"FP32\",\n            \"data\": feature_values,\n        }\n    ]\n}\n\nresponse = requests.post(\n    f\"{inference_url}/v2/models/fraud-detection/infer\",\n    json=payload,\n    timeout=10,\n)\n\nprint(f\"\\nResponse status: {response.status_code}\")\nresult = response.json()\n\n# Step 4: Parse the prediction\npredictions = result[\"outputs\"]\nlabel_output = next((o for o in predictions if o[\"name\"] == \"output_label\"), None)\nproba_output = next((o for o in predictions if o[\"name\"] == \"output_probability\"), None)\n\nif label_output:\n    is_fraud = label_output[\"data\"][0]\n    print(f\"\\n{'='*50}\")\n    if is_fraud == 1:\n        print(f\"FRAUD ALERT!\")\n    else:\n        print(f\"Legitimate transaction\")\n    if proba_output:\n        probas = proba_output[\"data\"]\n        # probas is a flat list: [p_class0, p_class1] for each sample\n        fraud_prob = probas[1] if len(probas) > 1 else probas[0]\n        print(f\"Fraud probability: {fraud_prob:.1%}\")\n    print(f\"{'='*50}\")\nelse:\n    print(f\"Raw output: {json.dumps(result, indent=2)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}