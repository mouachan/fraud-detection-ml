{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Detection avec Feature Store sur OpenShift AI\n",
    "\n",
    "Ce notebook illustre l'utilisation de **Feature Store** (Feast) sur **Red Hat OpenShift AI 3.2** pour un cas d'usage de **détection de fraude bancaire**.\n",
    "\n",
    "## Workflow\n",
    "1. Générer des données de transactions et profils clients\n",
    "2. Définir les features (entities, feature views, on-demand features)\n",
    "3. Enregistrer les features dans le Feature Store (`feast apply`)\n",
    "4. Matérialiser les features dans le online store (PostgreSQL)\n",
    "5. Récupérer les features historiques pour entraîner un modèle\n",
    "6. Prédiction en temps réel via le online store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation des dépendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install feast[postgres] scikit-learn pandas pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration du Feature Store\n",
    "\n",
    "Récupération de la config client depuis le ConfigMap déployé par l'opérateur Feast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport yaml\n\nFEATURE_REPO_PATH = \"../feature_repo\"\n\n# Configuration pour se connecter au Feature Store déployé sur OpenShift.\n# En workbench RHOAI, cette config est fournie par le ConfigMap\n# 'feast-fraud-features-client'.\n# Ici on utilise une config locale pour la démo qui pointe directement\n# vers les backends (PostgreSQL online store, DuckDB offline, S3 registry).\n#\n# Les credentials sont lus depuis des variables d'environnement.\n# Sur un workbench RHOAI, configurez-les via les env vars du notebook\n# ou via un Secret monté dans le pod.\n\nfeature_store_config = {\n    \"project\": \"fraud_detection\",\n    \"provider\": \"local\",\n    \"offline_store\": {\n        \"type\": \"duckdb\",\n    },\n    \"online_store\": {\n        \"type\": \"postgres\",\n        \"host\": os.getenv(\"POSTGRES_HOST\", \"postgres.fraud-detection-ml.svc.cluster.local\"),\n        \"port\": int(os.getenv(\"POSTGRES_PORT\", \"5432\")),\n        \"database\": os.getenv(\"POSTGRES_DB\", \"feast_db\"),\n        \"user\": os.getenv(\"POSTGRES_USER\", \"feast_user\"),\n        \"password\": os.getenv(\"POSTGRES_PASSWORD\"),\n    },\n    \"registry\": {\n        \"path\": os.getenv(\"FEAST_REGISTRY_PATH\", \"s3://feast-registry/registry.db\"),\n        \"registry_type\": \"file\",\n    },\n    \"entity_key_serialization_version\": 3,\n}\n\nif not feature_store_config[\"online_store\"][\"password\"]:\n    raise ValueError(\n        \"POSTGRES_PASSWORD non défini. \"\n        \"Configurez la variable d'environnement avant d'exécuter ce notebook.\"\n    )\n\n# Écrire le fichier feature_store.yaml\nconfig_path = os.path.join(FEATURE_REPO_PATH, \"feature_store.yaml\")\nwith open(config_path, \"w\") as f:\n    yaml.dump(feature_store_config, f, default_flow_style=False)\n\nprint(f\"Configuration écrite dans {config_path}\")\n# Afficher la config sans le mot de passe\nsafe_config = feature_store_config.copy()\nsafe_config[\"online_store\"] = {\n    **safe_config[\"online_store\"],\n    \"password\": \"****\"\n}\nprint(yaml.dump(safe_config, default_flow_style=False))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Génération des données de démo\n",
    "\n",
    "On simule des données de profils clients et de statistiques de transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "np.random.seed(42)\n",
    "N_CUSTOMERS = 500\n",
    "\n",
    "now = pd.Timestamp.now()\n",
    "\n",
    "# --- Profils clients ---\n",
    "customer_ids = [f\"C{str(i).zfill(5)}\" for i in range(N_CUSTOMERS)]\n",
    "customer_profiles = pd.DataFrame({\n",
    "    \"customer_id\": customer_ids,\n",
    "    \"age\": np.random.randint(18, 75, N_CUSTOMERS),\n",
    "    \"country\": np.random.choice([\"FR\", \"US\", \"UK\", \"DE\", \"ES\"], N_CUSTOMERS),\n",
    "    \"account_age_days\": np.random.randint(30, 3650, N_CUSTOMERS),\n",
    "    \"credit_limit\": np.round(np.random.uniform(1000, 50000, N_CUSTOMERS), 2),\n",
    "    \"num_cards\": np.random.randint(1, 5, N_CUSTOMERS),\n",
    "    \"event_timestamp\": [now - pd.Timedelta(hours=np.random.randint(1, 48)) for _ in range(N_CUSTOMERS)],\n",
    "    \"created_timestamp\": [now] * N_CUSTOMERS,\n",
    "})\n",
    "\n",
    "# --- Statistiques de transactions ---\n",
    "transaction_stats = pd.DataFrame({\n",
    "    \"customer_id\": customer_ids,\n",
    "    \"avg_transaction_amount_30d\": np.round(np.random.uniform(20, 500, N_CUSTOMERS), 2),\n",
    "    \"num_transactions_7d\": np.random.randint(0, 50, N_CUSTOMERS),\n",
    "    \"num_transactions_1d\": np.random.randint(0, 15, N_CUSTOMERS),\n",
    "    \"max_transaction_amount_7d\": np.round(np.random.uniform(50, 5000, N_CUSTOMERS), 2),\n",
    "    \"num_foreign_transactions_30d\": np.random.randint(0, 10, N_CUSTOMERS),\n",
    "    \"num_declined_transactions_7d\": np.random.randint(0, 5, N_CUSTOMERS),\n",
    "    \"event_timestamp\": [now - pd.Timedelta(hours=np.random.randint(1, 24)) for _ in range(N_CUSTOMERS)],\n",
    "    \"created_timestamp\": [now] * N_CUSTOMERS,\n",
    "})\n",
    "\n",
    "# Sauvegarder en Parquet\n",
    "data_dir = os.path.join(FEATURE_REPO_PATH, \"data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "customer_profiles.to_parquet(os.path.join(data_dir, \"customer_profiles.parquet\"))\n",
    "transaction_stats.to_parquet(os.path.join(data_dir, \"transaction_stats.parquet\"))\n",
    "\n",
    "print(f\"{N_CUSTOMERS} profils clients générés\")\n",
    "print(f\"{N_CUSTOMERS} statistiques de transactions générées\")\n",
    "customer_profiles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_stats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Définition des Features\n",
    "\n",
    "Les features sont définies dans `feature_repo/features.py`. Voici un aperçu :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les définitions de features\n",
    "with open(os.path.join(FEATURE_REPO_PATH, \"features.py\"), \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Enregistrement des features (`feast apply`)\n",
    "\n",
    "On enregistre les entités, feature views et on-demand features dans le registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feast import FeatureStore\n",
    "\n",
    "# Initialiser le Feature Store\n",
    "store = FeatureStore(repo_path=FEATURE_REPO_PATH)\n",
    "\n",
    "print(\"Projet Feast :\", store.project)\n",
    "print(\"Applying feature definitions...\")\n",
    "store.apply(\n",
    "    [\n",
    "        store.repo.feature_views[0].entities[0] if store.repo.feature_views else None,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer toutes les définitions via la CLI feast\n",
    "!cd {FEATURE_REPO_PATH} && feast apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier les objets enregistrés\n",
    "store = FeatureStore(repo_path=FEATURE_REPO_PATH)\n",
    "\n",
    "print(\"=== Entités ===\")\n",
    "for entity in store.list_entities():\n",
    "    print(f\"  - {entity.name}\")\n",
    "\n",
    "print(\"\\n=== Feature Views ===\")\n",
    "for fv in store.list_feature_views():\n",
    "    print(f\"  - {fv.name} ({len(fv.features)} features, TTL={fv.ttl})\")\n",
    "    for feature in fv.features:\n",
    "        print(f\"      {feature.name}: {feature.dtype}\")\n",
    "\n",
    "print(\"\\n=== On-Demand Feature Views ===\")\n",
    "for odfv in store.list_on_demand_feature_views():\n",
    "    print(f\"  - {odfv.name}\")\n",
    "    for feature in odfv.features:\n",
    "        print(f\"      {feature.name}: {feature.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Matérialisation dans le Online Store (PostgreSQL)\n",
    "\n",
    "On charge les features dans PostgreSQL pour le serving temps réel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Matérialiser les features des 2 derniers jours dans le online store\n",
    "store.materialize_incremental(end_date=datetime.utcnow())\n",
    "print(\"Matérialisation terminée dans PostgreSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Récupération des features historiques (Training)\n",
    "\n",
    "On récupère les features depuis le **offline store** (DuckDB) pour entraîner un modèle de détection de fraude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simuler un jeu de données de transactions labellisées\n",
    "N_TRANSACTIONS = 2000\n",
    "\n",
    "entity_df = pd.DataFrame({\n",
    "    \"customer_id\": np.random.choice(customer_ids, N_TRANSACTIONS),\n",
    "    \"event_timestamp\": [now - pd.Timedelta(hours=np.random.randint(1, 24)) for _ in range(N_TRANSACTIONS)],\n",
    "    \"transaction_amount\": np.round(np.random.exponential(200, N_TRANSACTIONS), 2),\n",
    "    \"is_foreign_transaction\": np.random.choice([0, 1], N_TRANSACTIONS, p=[0.85, 0.15]),\n",
    "})\n",
    "\n",
    "# Liste des features à récupérer\n",
    "feature_refs = [\n",
    "    \"customer_profile:age\",\n",
    "    \"customer_profile:account_age_days\",\n",
    "    \"customer_profile:credit_limit\",\n",
    "    \"customer_profile:num_cards\",\n",
    "    \"transaction_stats:avg_transaction_amount_30d\",\n",
    "    \"transaction_stats:num_transactions_7d\",\n",
    "    \"transaction_stats:num_transactions_1d\",\n",
    "    \"transaction_stats:max_transaction_amount_7d\",\n",
    "    \"transaction_stats:num_foreign_transactions_30d\",\n",
    "    \"transaction_stats:num_declined_transactions_7d\",\n",
    "    \"fraud_risk_features:amount_ratio_to_avg\",\n",
    "    \"fraud_risk_features:amount_ratio_to_max\",\n",
    "    \"fraud_risk_features:risk_score\",\n",
    "]\n",
    "\n",
    "# Récupérer les features historiques depuis l'offline store\n",
    "print(\"Récupération des features historiques...\")\n",
    "training_df = store.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=feature_refs,\n",
    ").to_df()\n",
    "\n",
    "print(f\"Dataset d'entraînement : {training_df.shape[0]} lignes, {training_df.shape[1]} colonnes\")\n",
    "training_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Entraînement du modèle de détection de fraude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Générer des labels de fraude simulés\n",
    "# Les transactions avec un risk_score élevé ont plus de chances d'être frauduleuses\n",
    "training_df = training_df.dropna()\n",
    "fraud_probability = 1 / (1 + np.exp(-(training_df[\"risk_score\"] - 1.5) * 3))\n",
    "training_df[\"is_fraud\"] = (np.random.random(len(training_df)) < fraud_probability).astype(int)\n",
    "\n",
    "print(f\"Distribution des fraudes :\")\n",
    "print(training_df[\"is_fraud\"].value_counts())\n",
    "print(f\"Taux de fraude : {training_df['is_fraud'].mean():.2%}\")\n",
    "\n",
    "# Préparer les features pour le modèle\n",
    "model_features = [\n",
    "    \"age\", \"account_age_days\", \"credit_limit\", \"num_cards\",\n",
    "    \"avg_transaction_amount_30d\", \"num_transactions_7d\", \"num_transactions_1d\",\n",
    "    \"max_transaction_amount_7d\", \"num_foreign_transactions_30d\",\n",
    "    \"num_declined_transactions_7d\",\n",
    "    \"transaction_amount\", \"is_foreign_transaction\",\n",
    "    \"amount_ratio_to_avg\", \"amount_ratio_to_max\", \"risk_score\",\n",
    "]\n",
    "\n",
    "X = training_df[model_features]\n",
    "y = training_df[\"is_fraud\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Entraîner un Random Forest\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Évaluer\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"\\n=== Rapport de classification ===\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Légitime\", \"Fraude\"]))\n",
    "\n",
    "print(\"=== Matrice de confusion ===\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importance des features\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "importances = pd.Series(clf.feature_importances_, index=model_features).sort_values(ascending=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "importances.plot(kind=\"barh\", ax=ax)\n",
    "ax.set_title(\"Importance des features pour la détection de fraude\")\n",
    "ax.set_xlabel(\"Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Prédiction en temps réel via le Online Store\n",
    "\n",
    "Simulation d'une transaction entrante : on récupère les features du client depuis le **online store** (PostgreSQL) en temps réel, puis on applique le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simuler une transaction suspecte\n",
    "test_customer = \"C00042\"\n",
    "transaction = {\n",
    "    \"transaction_amount\": 4500.00,  # montant élevé\n",
    "    \"is_foreign_transaction\": 1,     # transaction à l'étranger\n",
    "}\n",
    "\n",
    "print(f\"Transaction entrante pour le client {test_customer}:\")\n",
    "print(f\"  Montant : {transaction['transaction_amount']} EUR\")\n",
    "print(f\"  Transaction étrangère : {'Oui' if transaction['is_foreign_transaction'] else 'Non'}\")\n",
    "\n",
    "# Récupérer les features en temps réel depuis PostgreSQL\n",
    "online_features = store.get_online_features(\n",
    "    entity_rows=[\n",
    "        {\n",
    "            \"customer_id\": test_customer,\n",
    "            **transaction,\n",
    "        }\n",
    "    ],\n",
    "    features=feature_refs,\n",
    ").to_dict()\n",
    "\n",
    "print(\"\\nFeatures récupérées depuis le online store (PostgreSQL) :\")\n",
    "for key, values in online_features.items():\n",
    "    if key != \"customer_id\":\n",
    "        print(f\"  {key}: {values[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construire le vecteur de features pour la prédiction\n",
    "feature_vector = {}\n",
    "feature_vector.update(online_features)\n",
    "feature_vector.update({k: [v] for k, v in transaction.items()})\n",
    "\n",
    "predict_df = pd.DataFrame(feature_vector)\n",
    "\n",
    "# Ne garder que les features du modèle (dans le bon ordre)\n",
    "available_features = [f for f in model_features if f in predict_df.columns]\n",
    "predict_input = predict_df[available_features]\n",
    "\n",
    "# Prédiction\n",
    "prediction = clf.predict(predict_input)[0]\n",
    "probability = clf.predict_proba(predict_input)[0]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "if prediction == 1:\n",
    "    print(f\"ALERTE FRAUDE - Probabilité : {probability[1]:.1%}\")\n",
    "    print(\"Action : Transaction bloquée pour vérification\")\n",
    "else:\n",
    "    print(f\"Transaction légitime - Probabilité fraude : {probability[1]:.1%}\")\n",
    "    print(\"Action : Transaction approuvée\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Résumé de l'architecture\n",
    "\n",
    "```\n",
    "                    OpenShift AI 3.2\n",
    "    ┌─────────────────────────────────────────┐\n",
    "    │                                         │\n",
    "    │   ┌─────────────┐   ┌───────────────┐  │\n",
    "    │   │  Notebook    │   │  Feature Store │  │\n",
    "    │   │  (Workbench) │──▶│  (Feast)       │  │\n",
    "    │   └─────────────┘   └───────┬───────┘  │\n",
    "    │                             │           │\n",
    "    │              ┌──────────────┼────────┐  │\n",
    "    │              ▼              ▼        ▼  │\n",
    "    │   ┌──────────────┐ ┌──────────┐ ┌────┐ │\n",
    "    │   │ Offline Store │ │  Online  │ │ S3 │ │\n",
    "    │   │   (DuckDB)   │ │  Store   │ │Reg.│ │\n",
    "    │   │              │ │(Postgres)│ │    │ │\n",
    "    │   │  Training    │ │ Serving  │ │Meta│ │\n",
    "    │   └──────────────┘ └──────────┘ └────┘ │\n",
    "    │                                         │\n",
    "    └─────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Composants\n",
    "\n",
    "| Composant | Technologie | Usage |\n",
    "|-----------|-------------|-------|\n",
    "| Offline Store | DuckDB | Features historiques pour le training |\n",
    "| Online Store | PostgreSQL | Features temps réel pour l'inférence |\n",
    "| Registry | S3 (MinIO) | Métadonnées des features |\n",
    "| Feature Server | Feast (RHOAI) | API gRPC/REST pour servir les features |\n",
    "| Notebook | Workbench RHOAI | Développement et expérimentation |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}